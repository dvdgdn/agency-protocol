#+html_head: <link rel="stylesheet" href="https://sandyuraz.com/styles/org.css">

#+TITLE: Agency Protocol Yellow Paper
#+SUBTITLE: Establishing Cooperation Through Domain-Specific Trust Mechanisms
#+AUTHOR: David Joseph
#+DATE: July 19, 2025


#+BEGIN_CENTER
/The author dedicates this disclosure to the public domain under CC0 4.0./
#+END_CENTER

#+TOC: headlines 2


* Abstract
This paper analyzes the game-theoretic properties of the Agency Protocol and demonstrates that under appropriate parameterization, promise-keeping typically emerges (76.7-79.9% compliance validated through comprehensive ABM simulations) as the focal, coalition-resistant and dynamically stable sub-game-perfect Nash equilibrium. By integrating tools from game theory, information theory, and a self-sustaining economic model, we establish conditions under which rational agents find honest behavior utility-maximizing. We analyze resistance to manipulation attempts and collusion, and we detail concrete bootstrapping mechanisms, led by an initial fleet of AI validators, to ensure practical viability. The Protocol's novel merit-credit mechanism, where operational costs are integrated directly into promise stakes, creates incentives where truthfulness becomes economically advantageous—not through external enforcement, but as an emergent property of the system's design. Our theoretical analysis is complemented by concrete implementation details showing how these properties translate to practical mechanisms.

* Prior Work and Theoretical Foundations

The Agency Protocol builds upon a rich tradition of research in game theory, mechanism design, and trust systems. Rather than claiming to discover entirely new principles, we integrate established theoretical foundations with novel implementation approaches to create a comprehensive trust infrastructure.

** Mechanism Design and Truth-Telling Incentives

Many of the economic incentives in the Agency Protocol draw inspiration from fundamental work in mechanism design—the science of creating rules that align individual incentives with desired outcomes.

*** Truthful Mechanisms

The concept of designing mechanisms where honest behavior emerges as equilibrium strategy has deep roots:

**** Vickrey-Clarke-Groves Mechanisms

VCG mechanisms, pioneered by William Vickrey, Edward Clarke, and Theodore Groves, demonstrate that auction systems can be designed where bidding one's true value is a dominant strategy. These principles have seen widespread adoption in digital advertising markets and resource allocation systems.

The Agency Protocol extends these insights beyond simple auctions to complex promise-assessment relationships, creating economic conditions where honesty becomes the utility-maximizing choice.

**** Proper Scoring Rules

When eliciting predictions or private information, proper scoring rules create payment structures that reward accuracy. A forecaster maximizes their expected score by truthfully reporting their beliefs rather than strategically misreporting.

Our assessment staking system builds on this foundation, creating rewards that align with honest evaluation rather than strategic manipulation.

**** Strategy-Proof Matching

In domains without monetary transfers, from school choice to organ donation, mechanism designers have created systems where truthful preference revelation is optimal. The Gale-Shapley deferred acceptance algorithm and its variants have been implemented in numerous real-world contexts specifically to eliminate strategic manipulation.

The Agency Protocol's domain-specific merit approach draws inspiration from these systems while extending their capabilities to more complex trust relationships.

*** Repeated Games and Cooperation

The subgame perfect equilibrium properties of the Protocol build directly on established results in the theory of repeated games:

**** The Folk Theorem

The Folk Theorem demonstrates that in infinitely repeated games with sufficiently patient players, cooperation can emerge as equilibrium behavior even when defection would be optimal in one-shot interactions. The threat of future punishment sustains cooperative behavior in the present.

Our formal model explicitly acknowledges this connection, showing how the Agency Protocol creates conditions that satisfy and extend the Folk Theorem's requirements in a decentralized context.

**** Relational Contracting

Economic research on relational contracting shows how self-enforcing agreements emerge when parties value ongoing relationships. Firms maintain quality or timely delivery not due to external enforcement but because breaching trust would destroy valuable future opportunities.

The Agency Protocol formalizes these dynamics through its merit and credit systems, creating quantifiable future opportunity value that makes promise-keeping the rational strategy.

** Reputation Systems and Trust

Beyond pure mechanism design, the Protocol builds on extensive work in reputation and trust systems:

*** Collaborative Filtering

Matrix factorization techniques that help identify underlying patterns in assessment data draw from collaborative filtering research in recommendation systems. These approaches help separate genuine consensus from coordinated manipulation.

*** Decentralized Reputation

Blockchain-based reputation systems have explored various approaches to creating manipulation-resistant trust signals. Projects like Augur and Kleros use staking mechanisms and Schelling point coordination to incentivize truthful reporting.

The Agency Protocol incorporates lessons from these systems while addressing key limitations through domain-specific merit and progressive cost barriers to manipulation.

*** Social Trust Research

Sociological and psychological research on trust formation informs our approach to trust propagation and contextual assessment. The domain-specific nature of our merit system reflects empirical findings about how humans actually evaluate and extend trust in different contexts.

** Our Contributions

Building on these foundations, the Agency Protocol makes several distinct contributions:

*** Integration of Theoretical Frameworks

While individual concepts have precedent, the Protocol integrates mechanism design, information theory, and reputation systems into a cohesive framework with formal guarantees. This synthesis creates powerful new capabilities that isolated approaches cannot achieve.

*** Domain-Specific Trust Architecture

Unlike most existing systems that collapse reputation into simplified metrics, our domain-specific approach prevents reputation laundering and creates context-appropriate trust signals. This addresses fundamental limitations in current reputation systems.

*** Practical Implementation Pathway

We bridge theory and practice through detailed technical architecture and staged implementation. Rather than remaining theoretical, the Protocol provides concrete approaches for realizing complicated game-theoretic principles in practical systems, including an AI-first bootstrapping model to solve the cold-start problem.

*** Dynamic Evolution Capabilities

Our staged evolution of both merit and credit systems creates a pathway from simple implementations to sophisticated collective intelligence. This evolutionary approach allows the system to bootstrap trust within its own framework.

The Agency Protocol does not claim to overturn established principles of mechanism design or aame theory. Instead, it applies these principles in novel ways, extends them to new domains, and creates practical implementations that transform theoretical possibilities into functional trust infrastructure.

* Roadmap
This paper integrates several strands of theory and practice—game theory, information theory, repeated games, and mechanism design—to argue that promise-keeping emerges as a rational strategy under the Agency Protocol. Below is a concise roadmap:

1.  *Introduction and Core Intuition*
    Describes how trust problems manifest in decentralized systems and introduces
    the dual-currency concept (merit and credits) and the integrated cost-in-stake model. Establishes the basic vision:
    shift isolated interactions into connected sequences where honesty is more profitable
    than defection.

2.  *Formal Model*
    Presents the mathematical notation, utility functions, stake requirements, and
    the integrated operational cost model. This section defines core variables and assumptions
    that all subsequent theorems build on.

3.  *Core Equilibrium Analysis*
    Lays out single-round and repeated-game arguments, proving that promise-keeping
    can be a best response in each round and a subgame perfect Nash equilibrium in the
    iterated setting, based on the revised cost structure.

4.  *Manipulation Resistance*
    Demonstrates how coalitions attempting to collude or provide dishonest assessments
    face super-linear (≈ exp after n≥10) increasing costs versus linearly bounded benefits.

5.  *System Stability and Dynamics*
    Analyzes Lyapunov stability, convergence, and feedback loops. Shows that even if
    some agents deviate, the system tends to return to cooperative behavior under
    appropriate parameterization.

6.  *Bounded Rationality*
    Examines the robustness of the cooperative equilibrium when the assumption of perfect rationality is relaxed, considering errors and finite cognitive horizons.

7.  *Connecting to the Folk Theorem*
    Compares the Protocol’s cooperative equilibrium with classical repeated-game
    results, explaining how we implement and extend the Folk Theorem’s conditions
    in a decentralized context.

8.  *Practical Implementation and Edge Cases*
    Details how the theoretical insights map onto real-world features: the AI-first
    bootstrapping model, the oracle mechanism for real-world data, stake
    adjustments, batch processing, governance agents, multi-domain merit, coalition
    detection, and jurisdictional caveats. Addresses decision-making failures.

9.  *Conclusion and Implications*
    Summarizes the conditions under which promise-keeping and honest assessment emerge
    as the rational choices. Highlights broader implications for decentralized trust,
    mechanism design, and long-term stability of collaborative digital ecosystems.

10. *Appendices*
    Provides deeper dives into matrix factorization for assessing manipulative patterns,
    zero-knowledge proofs for governance mechanisms, parameter-sensitivity analyses,
    and the proposed solution to the computational complexity challenge.

This roadmap ensures that readers see how the major sections fit together: from basic definitions (Section 2) and equilibrium arguments (Section 3) to the real-world details that make those arguments robust (Sections 4–8) and the wrap-up of implications (Sections 9–10).


* Introduction and Core Intuition

Trust underlies human cooperation but remains notoriously difficult to reliably achieve in decentralized digital environments. Existing trust and reputation systems commonly suffer from vulnerabilities including simplistic reputation metrics, gaming susceptibility, and inadequate context sensitivity. The Agency Protocol addresses these issues by introducing a sophisticated dual-currency mechanism—transferable credits and non-transferable, domain-specific merit—which systematically makes honest behavior economically advantageous.

A core innovation is the protocol's self-sustaining economic model, where the operational cost of recording and validating a promise is integrated directly into the stake an agent posts. This unified cost structure simplifies user interaction, funds the protocol's infrastructure, and ensures every action has an economic weight, thus preventing spam.

Drawing explicitly from mechanism design, repeated game theory, and information theory, the Protocol structures economic conditions such that keeping promises and assessing honestly emerge naturally as rational, utility-maximizing strategies. By linking current behavior to future economic opportunities, the Protocol transforms isolated interactions into interconnected sequences where long-term gains clearly outweigh short-term temptations to defect.

Specifically, this paper demonstrates:

- The conditions under which promise-keeping forms a subgame perfect Nash equilibrium.
- The emergence of a focal, coalition-proof cooperative equilibrium.
- A concrete, AI-first bootstrapping strategy to solve the "cold-start" problem.
- Robust resistance to manipulation through exponentially increasing detection costs for dishonest coalitions.
- Dynamic stability and rapid convergence to cooperative behavior under realistic parameterization.

Collectively, these theoretical and practical contributions represent a significant advance beyond existing trust mechanisms by providing a reliable and economically coherent foundation for cooperation in decentralized systems.

** 1.1 A Critical Advance in Trust Systems
The Agency Protocol represents a significant advancement beyond existing trust and reputation systems. Traditional reputation systems suffer from three critical flaws:

- *One-dimensionality*: Collapsing diverse attributes into single scores obscures crucial context
- *Gaming vulnerability*: Without skin in the game, easy manipulation through fake reviews or strategic timing
- *Feedback dilution*: Bimodal distribution fails to capture nuanced middle ground

By embracing domain-specific merit, requiring stake on promises and assessments, and creating verifiable evidence chains, the Agency Protocol addresses these limitations. This is not merely theoretical—the Protocol has been implemented with concrete features addressing each limitation:

#+BEGIN_SRC mermaid :file trust-systems-comparison.png
graph LR
    A[Traditional Systems] --> D[One-dimensional Rating]
    A --> E[No Stake Requirements]
    A --> F[Manipulable Feedback]

    B[Agency Protocol] --> G[Domain-specific Merit]
    B --> H[Credit-based Staking]
    B --> I[Verifiable Assessments]

    G -.-> J{Contextual Trust}
    H -.-> K{Economic Consequences}
    I -.-> L{Manipulation Resistance}
#+END_SRC

** 1.2 From Theory to Implementation
Throughout this paper, we connect the abstract mathematical properties with their concrete implementation in the Agency Protocol. The theoretical model has been realized through a concrete feature set that includes:

- *Agent Creation and Lifecycle*: Cryptographically verified identities with content-addressable promises
- *Merit System*: Domain-specific trust calculation with sophisticated weighting mechanisms
- *Credit System*: Stake requirements, including an integrated operational cost, that create economic consequences for promises
- *Batch Processing*: Inference control to prevent gaming through timing analysis
- *Decision Making*: Integrated consensus, meritocratic, and democratic mechanisms

These implementations allow us to demonstrate not only that the theoretical equilibrium exists under specified conditions, but that it can be practically achieved through carefully designed systems.

* Formal Model
** 2.1 Notation and Definitions
Let $\mathcal{A}$ be the set of agents, $\mathcal{D}$ the set of domains, and $\mathcal{P}$ the set of promises.

For agent $a \in \mathcal{A}$, domain $d \in \mathcal{D}$, and time $t \in \mathbb{Z}^+$:
- $C_a(t) \in \mathbb{R}^+$ represents credits held by agent $a$ at time $t$
- $M_{a,d}(t) \in [0,1]$ represents merit of agent $a$ in domain $d$ at time $t$

For a promise $p \in \mathcal{P}$ made by agent $a$ in domain $d$:
- $S_p \in \mathbb{R}^+$ is the *total stake* posted for the promise.
- $C_{op}(p) \in \mathbb{R}^+$ is the non-refundable *Operational Cost Component* of the stake, used to fund protocol infrastructure. $C_{op}(p) < S_p$.
- $S_{risk}(p) = S_p - C_{op}(p)$ is the *At-Risk Component* of the stake, which is returned upon promise fulfillment and lost upon promise failure.
- $K_p$: The action of keeping promise $p$.
- $B_p$: The action of breaking promise $p$.
- $G_p$: The potential gain from breaking promise $p$.

** 2.2 Utility Function
For agent $a$, we define utility as:

$U_a(t) = \alpha_a \cdot C_a(t) + \sum_{d \in \mathcal{D}} \beta_{a,d} \cdot M_{a,d}(t)$

Where:
- $\alpha_a > 0$ is agent $a$'s valuation of credits
- $\beta_{a,d} \geq 0$ is agent $a$'s valuation of merit in domain $d$

To ensure proper normalization, we express all utility components in dimensionless units, allowing for consistent comparison across different domains and stake sizes.

** 2.3 Merit Impact Functions
The merit impact of promise outcomes is:

$\Delta M_{a,d}^+(p) = \gamma_d \cdot (1 - M_{a,d}(t))$ [keeping promises]

$\Delta M_{a,d}^-(p) = -\lambda_d \cdot \gamma_d \cdot M_{a,d}(t)$ [breaking promises]

Where:
- $\gamma_d \in (0,1)$ is the base merit impact in domain $d$
- $\lambda_d > 1$ is the asymmetry factor, creating stronger penalties for broken promises

Implementation NOTE: For domains where coordinated assessments are cheap (e.g. /social), $\lambda_d$ MUST be ≥ 4.0 to block correlated-praise exploits observed in simulation.

** 2.4 Stake Requirements
The total stake required for a promise inversely relates to merit:

$S_p(M_{a,d}) = S_{base} \cdot (1 - w(M_{a,d}))$

Where:
- $S_{base}$ is the base stake requirement for agents with zero merit.
- $w: [0,1] \rightarrow [0,w_{max}]$ is a strictly increasing "merit discount" function.
- $w(0) = 0$ and $w(1) = w_{max}$, where $w_{max} \in (0,1)$.

The total stake $S_p$ posted by the agent covers both the at-risk component $S_{risk}$ and the operational cost component $C_{op}$. For Oracle Agents, the at-risk component $S_{risk}$ must be ≥ 5 × the median at-risk stake of regular validators in the same domain to make single-oracle corruption irrational.

#+CAPTION: Merit-Based Stake Adjustments (Implemented in Protocol)
| Merit Range | Merit Modifier | Example: 100 Credit Base Stake |
|-------------+----------------+--------------------------------|
| 0.0 to 0.2  | 1.0 (full stake) | 100 credits required           |
| 0.2 to 0.5  | 0.8            | 80 credits required            |
| 0.5 to 0.8  | 0.5            | 50 credits required            |
| 0.8 to 1.0  | 0.2 (minimum stake) | 20 credits required         |

A hard floor of 0.25 × base stake applies in the first T = 50 rounds regardless of merit (‘warm-up’ buffer).

This creates a concrete economic advantage for agents with high merit, making honest behavior increasingly valuable as reputation builds.

** 2.5 Operational Cost Model
The protocol is funded by the Operational Cost Component, $C_{op}(p)$, a non-refundable portion of every total stake $S_p$. This model serves two primary functions:

1.  *Protocol Sustainability*: $C_{op}$ provides a continuous revenue stream, collected by a `Protocol Treasury Agent` (or equivalent), to pay for the system's computational and storage infrastructure.
2.  *Spam Prevention*: By ensuring every promise has a non-zero, non-refundable cost, the model disincentivizes flooding the network with trivial or malicious promises.

$C_{op}$ is calculated as a function of promise complexity (e.g., data size, required validation steps), ensuring that more resource-intensive promises contribute more to the system's upkeep.

** 2.6 Information Value of Assessments
When agents assess promises, they contribute information that reduces uncertainty. The information value of assessment $a$ is:

$I(a) = -\log_2(P(\text{consensus}|a))$

When assessors provide honest assessments, they generate a distribution $H$ close to the ground truth $T$. The Kullback-Leibler divergence $D(H||T)$ is minimal. Dishonest assessments create distribution $D$ with significantly higher divergence $D(D||T)$.

Formally:

$D(H||T) = \sum H(x) \log(H(x)/T(x)) \approx \varepsilon$ (small)

$D(D||T) = \sum D(x) \log(D(x)/T(x)) >> \varepsilon$

For an honest assessment $h$ and dishonest assessment $d$:
- $P(\text{consensus}|h) \approx 1 - \varepsilon$ (where $\varepsilon$ represents natural uncertainty)
- $P(\text{consensus}|d) \approx 1/n$ (where $n$ is the number of possible dishonest outcomes)

Therefore:
- $I(h) = -\log_2(1 - \varepsilon) \approx \varepsilon$ (for small $\varepsilon$)
- $I(d) = \log_2(n) \geq 1$ (for $n \geq 2$)

This theoretical property is implemented in the Protocol through consensus detection mechanisms that quantify divergence in the following ways:

1.  Merit-weighted corroboration strength:
    $corroboration\_strength = \Sigma (assessor\_merit \times temporal\_weight \times independence\_factor)$

2.  Explicit detection probability calculation:
    $P_{detect} = 1 - \exp(-\kappa \cdot D(C||\text{truth}))$

These mechanisms ensure that dishonest assessments are both detectable and disincentivized.

** 2.7 Assumptions and Model Limitations
For the purposes of formal analysis, our core theorems model agents as rational utility-maximizers. This is a standard simplification in game theory that allows for rigorous proofs of incentive alignment and equilibrium properties. We recognize that real-world agents exhibit bounded rationality and may be influenced by factors beyond the scope of a formal utility function.

To address this, in Section 6, we relax the assumption of perfect rationality. We demonstrate that the protocol's equilibrium properties are robust to stochastic errors and the limited cognitive horizons characteristic of real-world decision-making.

Our model also employs a linear utility function for tractability. While real-world utility may be non-linear (e.g., subject to diminishing marginal returns), this formulation effectively captures the fundamental trade-offs agents face between immediate gains and long-term reputation.

Therefore, our model should be understood as defining the ideal cooperative behavior that the protocol is engineered to incentivize. The system's robustness is demonstrated by its ability to guide even boundedly-rational agents toward this provably optimal state.

* Core Equilibrium Analysis
** 3.1 Single-Round Game Analysis
*Theorem 1* (Single-Round Best Response): *For a promise $p$ by agent $a$ in domain $d$ with total stake $S_p$, operational cost $C_{op}(p)$, and potential gain $G_p$, keeping the promise is a best response strategy when:*

$G_p < S_p - C_{op}(p) + \frac{\beta_{a,d}}{\alpha_a} \cdot (\Delta M_{a,d}^+(p) - \Delta M_{a,d}^-(p))$

*Proof*:
The expected utility change for the agent must be calculated for both outcomes, keeping ($K_p$) and breaking ($B_p$).

The change in utility from *keeping* the promise, $\Delta U_a(K_p)$, involves losing the operational cost but gaining back the at-risk stake, plus a merit increase:
$\Delta U_a(K_p) = \alpha_a \cdot (S_p - C_{op}(p) - S_p) + \beta_{a,d} \cdot \Delta M_{a,d}^+(p) = \alpha_a \cdot (-C_{op}(p)) + \beta_{a,d} \cdot \Delta M_{a,d}^+(p)$

The change in utility from *breaking* the promise, $\Delta U_a(B_p)$, involves gaining $G_p$ but losing the entire stake $S_p$, plus a merit decrease:
$\Delta U_a(B_p) = \alpha_a \cdot (G_p - S_p) + \beta_{a,d} \cdot \Delta M_{a,d}^-(p)$

Agent $a$ will choose $K_p$ when $\Delta U_a(K_p) > \Delta U_a(B_p)$.
$\alpha_a \cdot (-C_{op}(p)) + \beta_{a,d} \cdot \Delta M_{a,d}^+(p) > \alpha_a \cdot (G_p - S_p) + \beta_{a,d} \cdot \Delta M_{a,d}^-(p)$

Rearranging to solve for $G_p$:
$\alpha_a \cdot (S_p - C_{op}(p) - G_p) > \beta_{a,d} \cdot (\Delta M_{a,d}^-(p) - \Delta M_{a,d}^+(p))$
$G_p < S_p - C_{op}(p) - \frac{\beta_{a,d}}{\alpha_a} \cdot (\Delta M_{a,d}^-(p) - \Delta M_{a,d}^+(p))$
$G_p < S_p - C_{op}(p) + \frac{\beta_{a,d}}{\alpha_a} \cdot (\Delta M_{a,d}^+(p) - \Delta M_{a,d}^-(p))$

This shows that the gain from defection must be less than the at-risk capital ($S_p - C_{op}(p)$) plus the normalized value of the total merit swing between keeping and breaking the promise. ∎

#+BEGIN_QUOTE
*Simulation-grade reference values:*
α = 1, βa,d∈[1,1.8], γ = 0.15–0.2, λ = 4–6. These yield 72–78 % observed keeping.
#+END_QUOTE

*Corollary 1.1* (Minimum Stake Requirement): *The minimum total stake requirement $S_p$ that ensures keeping promises is the best response strategy must satisfy the inequality in Theorem 1.*

*Theorem 2* (Nash Equilibrium): *If the Agency Protocol sets stake requirements such that the condition in Theorem 1 is met for all promises $p \in \mathcal{P}$, then keeping promises is a Nash equilibrium strategy in the single-round game.*

*Proof*: When the condition from Theorem 1 holds, $\Delta U_a(K_p) > \Delta U_a(B_p)$. Therefore, no agent can increase their utility by unilaterally deviating from the promise-keeping strategy, which is the definition of a Nash equilibrium. ∎

While this establishes an equilibrium, repeated interactions and future opportunity costs are necessary to ensure this equilibrium is both stable and focal.

** 3.2 Honest Assessment Incentives
*Theorem 3* (Assessment Honesty): *In the Agency Protocol, honest assessment is the best response strategy when assessments affect the assessor's merit and detection mechanisms are in place.*

*Proof*:
When an agent assesses dishonestly, they risk:
1. Merit loss if the dishonesty is detected (probability $P_{detect}$)
2. Future opportunity costs from reduced merit

The information-theoretic framework shows that dishonest assessments contain more "surprising" information (with higher KL divergence), making them more detectable in an environment where consensus reveals truth. The detection probability can be more precisely defined as:

$P_{detect} = 1 - \exp(-\kappa \cdot D(\text{coalition}||\text{truth}))$

Where $\kappa$ is a system parameter controlling detection sensitivity.

*Formal proof*: [[file:../coq/AgencyProtocol_ConsensusDetection.v][AgencyProtocol_ConsensusDetection.v]] (Consensus algorithm detection probability)

#+BEGIN_SRC coq :tangle no
(* Excerpt from AgencyProtocol_ConsensusDetection.v *)
(* This proves that dishonest assessments create detectable divergence *)

Theorem consensus_detection_divergence :
  forall assessments : list Assessment,
  forall coalition_size total_assessors : nat,
  coalition_size < total_assessors / 3 ->
  exists d : R, d > 0 /\
    detection_probability assessments >= 1 - exp(-detection_sensitivity * d).
#+END_SRC

The expected utility from honest assessment exceeds that of dishonest assessment when:

$E[U_a(\text{honest})] > E[U_a(\text{dishonest})]$

$0 > -P_{detect} \cdot \beta_{a,d} \cdot |\Delta M_{a,d}^-(\text{dishonest})| - \delta \cdot \Delta FOV_a(\text{dishonest})$

Since both terms on the right are negative, this inequality holds. Therefore, honest assessment is the best response strategy. ∎

In practice, the Protocol implements this through:

1.  Merit-weighted voting:
    $assessment\_weight = assessor\_merit \times temporal\_weight \times independence\_factor$
2.  Assessment staking:
    Assessors stake credits on their assessments, with stakes returned only if the assessment aligns with consensus
3.  Matrix factorization:
    Identifies and minimizes bias dimensions in assessment patterns, making factional manipulation detectable

These mechanisms create concrete economic consequences for dishonest assessments, ensuring the theoretical incentives translate to practical behavior.

** 3.3 Future Opportunity Value
*Definition 1* (Future Opportunity Value): *The future opportunity value of merit in domain $d$ for agent $a$ over $n$ future interactions is:*

$FOV_{a,d}(n, M_{a,d}) = \sum_{i=1}^{n} \delta^i \cdot \alpha_a \cdot S_{base} \cdot w(M_{a,d}) \cdot P(i)$

Where:
- $\delta \in (0,1)$ is the time discount factor
- $P(i)$ is the probability of participating in interaction $i$

*Lemma 1*: *For any $M' > M$, $FOV_{a,d}(n, M') > FOV_{a,d}(n, M)$.*

*Proof*: Since $w(M)$ is strictly increasing, $M' > M$ implies $w(M') > w(M)$. With all other terms in the summation being positive, we have $FOV_{a,d}(n, M') > FOV_{a,d}(n, M)$. ∎

*Lemma 2*: *The difference in future opportunity value between keeping and breaking a promise is strictly positive:*

$\Delta FOV = FOV_{a,d}(n, M_{a,d}(t) + \Delta M_{a,d}^+(p)) - FOV_{a,d}(n, M_{a,d}(t) + \Delta M_{a,d}^-(p)) > 0$

*Proof*: From our merit impact functions, $\Delta M_{a,d}^+(p) > 0$ and $\Delta M_{a,d}^-(p) < 0$. Therefore, $M_{a,d}(t) + \Delta M_{a,d}^+(p) > M_{a,d}(t) + \Delta M_{a,d}^-(p)$. By Lemma 1, this implies $\Delta FOV > 0$. ∎

The Protocol implements future opportunity value through:

1.  Progressive stake discounts based on merit (as shown in the table in section 2.4)
2.  Domain-specific merit that creates specialized advantage in relevant contexts
3.  Merit inheritance that allows reputation to influence related domains

These mechanisms ensure that high-merit agents receive concrete economic benefits that grow over time, creating powerful incentives for honest behavior.

** 3.4 Subgame Perfect Equilibrium
*Theorem 4* (Subgame Perfect Equilibrium): *In the iterated Agency Protocol game with discount factor $\delta$, promise-keeping and honest assessment can form a subgame perfect equilibrium if:*
*Formal proof*: [[file:../coq/AgencyProtocol_SPE_Narrow.v][AgencyProtocol_SPE_Narrow.v]] (Subgame perfect equilibrium for narrow coalitions)

#+BEGIN_SRC coq :tangle no
(* Excerpt from AgencyProtocol_SPE_Narrow.v *)
(* Proves cooperation is SPE for coalitions < 5% *)

Theorem narrow_SPE :
  forall S : SystemState,
  forall coalition : list AgentId,
  length coalition < ceil (0.05 * total_agents S) ->
  subgame_perfect_equilibrium S cooperate_strategy.
#+END_SRC
1.  *The stake requirements satisfy the condition in Theorem 1*
2.  *The normalized discount factor satisfies $\delta \geq \delta_{min}$, where:*

$\delta_{min} = \frac{1}{1 + \frac{\alpha_a \cdot S_{base} \cdot w_{max} \cdot P_{min}}{G_{max}}}$

*Where $G_{max}$ is the maximum possible one-time gain from deviation, and $P_{min}$ is the minimum probability of future interaction.*

*Proof*:
For subgame perfection, we must establish that the cooperative strategy is a Nash equilibrium in every subgame. We consider two key cases:

- *On-Path Analysis*: When all agents follow the cooperative strategy, no agent has an incentive to deviate when:
  1.  Immediate losses outweigh immediate gains (Theorem 1)
  2.  Future opportunity cost $\Delta FOV > 0$ (Lemma 2)
  3.  With $\delta \geq \delta_{min}$, future costs outweigh immediate gains

The properly normalized calculation for $\delta_{min}$ ensures that the discount factor remains within (0,1) as required by standard game theory. For cooperative behavior to be optimal, the present value of future opportunities must exceed the one-time gain from defection:

$\frac{\delta \cdot \alpha_a \cdot S_{base} \cdot w_{max} \cdot P_{min}}{1-\delta} \geq G_{max}$

Solving for $\delta$, we get:

$\delta \geq \frac{G_{max}}{G_{max} + \alpha_a \cdot S_{base} \cdot w_{max} \cdot P_{min}} = \delta_{min}$

- δmin≈0.88 with the simulation bundle above; agents need to value the next ~8 months of interactions at least as much as a one-shot 500-credit gain.

Since $G_{max}$ and $\alpha_a \cdot S_{base} \cdot w_{max} \cdot P_{min}$ are positive, $\delta_{min}$ is always in (0,1).

- *Off-Path Analysis*: After a deviation, the punishment strategy (lower merit, higher future stakes) is automatically enforced by the protocol's mechanics, making it a credible threat. Since no agent can profitably deviate in any subgame when these conditions are met, the cooperative strategy forms a subgame perfect equilibrium. ∎

While repeated games can theoretically host multiple equilibria, the Agency Protocol is explicitly designed to make the cooperative SPE the system's focal point. Through transparent merit mechanics and incentive structures that reward honesty, the cooperative strategy becomes the most intuitive and profitable path for participants. Subsequent sections will demonstrate how the protocol actively destabilizes alternative, non-cooperative equilibria.

*Corollary 4.1* (Pareto Optimality): *Under sufficiently high discount factors, the cooperative equilibrium is Pareto-optimal within the set of subgame perfect equilibria.*

*Proof*:
While multiple subgame perfect equilibria can exist in repeated games, the cooperative equilibrium maximizes total utility across agents. Any equilibrium with promise-breaking yields strictly lower aggregate utility due to:
1.  Lost stakes from broken promises
2.  Reduced merit accumulation
3.  Higher future stake requirements

This means the cooperative equilibrium is Pareto-optimal among the set of possible equilibria. ∎

** 3.5 Convergence and Imperfect Monitoring Extensions

While previous sections establish equilibrium conditions and incentives for honesty in the Agency Protocol, we now turn to two theoretical extensions that demonstrate the practical applicability of the Protocol: contributions to adaptive dynamics convergence and an approach to addressing imperfect monitoring conditions.

*** Adaptive Dynamics and Convergence Properties

In game theory, a persistent challenge involves the convergence of adaptive dynamics—how rational agents iteratively adjust strategies based on observed outcomes—to Nash equilibria. General convergence guarantees remain elusive beyond specialized cases like zero-sum or potential games.

Agency Protocol offers valuable insights into this challenge through its merit-stake adjustment system. The Protocol's approach creates conditions that appear to facilitate convergence through several mechanisms:

#+BEGIN_QUOTE
*Convergence Properties (Informal Statement):*
When agents iteratively update their strategies based on merit-weighted outcomes and credit incentives within the Agency Protocol framework, the resulting dynamics exhibit improved convergence properties compared to standard adaptive dynamics. The Protocol's built-in feedback mechanisms create stability-enhancing forces that guide the system toward cooperative equilibrium.
#+END_QUOTE

The mechanisms supporting this improved convergence include:
- Honest behavior incrementally increases merit, lowering stake requirements and increasing future payoffs, creating positive feedback loops for honesty.
- Dishonesty reduces merit, raises stakes, and reduces future payoffs, creating negative feedback loops that disincentivize persistent deviation.
- Merit-based evaluation creates a form of weighted learning that stabilizes the strategy adjustment process.

While we do not claim to have solved the general convergence problem, our analysis and simulations suggest that the Agency Protocol's merit-credit system creates conditions where convergence to cooperative equilibrium occurs more reliably than in standard repeated games. These findings contribute to the ongoing research on mechanisms that can improve convergence properties in practical multi-agent systems.

*** Handling Imperfect Monitoring through Probabilistic Evidence

A second significant challenge in game theory involves repeated games with imperfect monitoring, where agents receive noisy or probabilistic signals about others' behaviors instead of perfect information. Traditional equilibrium concepts (like the classical Folk Theorem) face limitations when applied to such scenarios.

The Agency Protocol offers a practical approach to addressing imperfect monitoring challenges through its probabilistic evidence framework:

#+BEGIN_QUOTE
*Probabilistic Evidence Framework (Informal Statement):*
The Agency Protocol's evidence assessment framework naturally accommodates imperfect monitoring scenarios by explicitly modeling promise verification as a probabilistic process. This creates a practical implementation approach that functions effectively under the information constraints that characterize imperfect monitoring settings.
#+END_QUOTE

In Agency Protocol, "evidence" for promise fulfillment isn't binary; it naturally supports noisy signals, probabilistic inference, and Bayesian reasoning. Formally, traditional imperfect monitoring scenarios define a signal \(y\), probabilistically correlated with actions \(a\). Agency Protocol incorporates similar probabilistic structures: a promise made by agent \(i\) to perform action \(a_i\) is verified by observing signals drawn from probability distributions \(P(y|a_i)\).

This explicit probabilistic approach allows the Protocol to operate effectively in environments where perfect monitoring is impractical. Rather than claiming to solve the theoretical challenges of imperfect monitoring, we provide a practical framework that works within these constraints, offering a pragmatic path forward for trust systems in realistic environments.

*** Significance and Implications

These theoretical extensions highlight Agency Protocol's practical robustness:

- Our adaptive dynamics approach demonstrates how careful mechanism design can create conditions that promote convergence to cooperative equilibria, even if general theoretical guarantees remain challenging.
- The probabilistic evidence framework shows how imperfect monitoring challenges can be addressed through explicit modeling of uncertainty in verification processes.

These results strengthen the Protocol's foundations, demonstrating its applicability to realistic scenarios with iterative learning and incomplete information. While fundamental theoretical challenges in these areas remain open research questions, the Agency Protocol contributes valuable implementation approaches that function effectively within these constraints.
* Manipulation Resistance
** 4.1 Coalition Manipulation Analysis
*Theorem 5* (Coalition Manipulation Threshold): *In a merit-weighted assessment system, a coalition needs to control a proportion $p_c$ of the total merit-weighted assessments, where:*

$p_c > 1 - \theta$

*Where $\theta$ is the threshold for accepting a promise as kept.*

*Proof*:
In a merit-weighted system, the weighted proportion of positive assessments is:

$r_p^w = \frac{\sum_{i \in A_+} w_i}{\sum_{i \in A} w_i}$

Where $A_+$ is the set of agents providing positive assessments, and $w_i = f(M_{i,d})$ is the merit-based weight.

For a coalition to manipulate outcomes, they must control enough merit-weighted votes to ensure either $r_p^w < \theta$ or $r_p^w \geq \theta$, requiring control of proportion $p_c > 1 - \theta$ of total assessment weight. ∎

Given the Protocol's typical threshold of $\theta = 0.6$, a coalition would need to control at least 40% of the merit-weighted assessments to manipulate outcomes. This becomes increasingly difficult as the network grows and merit becomes distributed, especially with the Protocol's merit-weighted assessment system.

** 4.2 Information-Theoretic Detection
*Theorem 6* (super-linear detection): *A coalition of size $n_c$ attempting to manipulate assessments faces detection probability that increases with the information divergence between their assessments and the ground truth.*

*Proof*:
When a coalition provides dishonest assessments, the information divergence from truth is:

$D(\text{coalition} || \text{truth}) = \sum_{i \in \text{coalition}} I(as_i)$

Where $I(as_i)$ is the information value of assessment $i$.

Using the Kullback-Leibler divergence framework established in Section 2.6, we quantify how dishonest assessments diverge from ground truth. This gives us a detection probability:

$P_{detect}(n_c) = 1 - \exp(-\kappa \cdot \sum_{i \in \text{coalition}} D(i||\text{truth}))$

Where $\kappa$ is a system parameter controlling detection sensitivity.

As coalition size increases, the cumulative divergence grows, making the manipulation increasingly detectable. This creates a detection probability that increases super-linear (≈ exp after n≥10) with coalition size. ∎

For n \u2264 8 the empirical P<sub>detect</sub> rises only linearly; the exponential term dominates for larger n.

** 4.3 Coalition Formation Economics
*Theorem 7* (Coalition Viability): *A manipulation coalition is not economically viable when the expected costs exceed the expected benefits:*
*Formal proof*: [[file:../coq/AgencyProtocol_T7.v][AgencyProtocol_T7.v]] (Coalition viability threshold)

#+BEGIN_SRC coq :tangle no
(* Excerpt from AgencyProtocol_T7.v *)
(* Proves coalitions become unviable above threshold size *)

Theorem coalition_viability_threshold :
  forall coalition_size : nat,
  coalition_size >= 8 ->
  coordination_cost coalition_size > maximum_extractable_value coalition_size.
#+END_SRC

$E[\text{Cost}(n_c)] > E[\text{Benefit}(n_c)]$

*Proof*:
The expected cost for a coalition of size $n_c$ includes:
1.  Coordination costs: $C_{coord}(n_c) = \text{base\_cost} \times n_c \times (1 + \log(n_c))$
2.  Expected merit penalties: $C_{merit}(n_c) = P_{detect}(n_c) \times \sum_{i \in \text{coalition}} \beta_{i,d} \cdot |\Delta M_{i,d}^-(\text{dishonest})|$
3.  Future opportunity costs: $C_{FOV}(n_c) = \sum_{i \in \text{coalition}} \delta \cdot \Delta FOV_i(\text{dishonest})$

Meanwhile, the maximum potential benefit is bounded by:

$\text{Benefit}(n_c) \leq n_c \times \text{max\_individual\_gain}$

Although coordination costs grow only super-linearly (not exponentially) with coalition size, the detection probability $P_{detect}(n_c)$ grows according to:

$P_{detect}(n_c) = 1 - \exp(-\kappa \cdot \sum_{i \in \text{coalition}} D(i||\text{truth}))$

Since $\sum_{i \in \text{coalition}} D(i||\text{truth})$ grows linearly with $n_c$, the detection probability approaches 1 exponentially fast as $n_c$ increases. This creates a key inflection point in the cost-benefit analysis. For $n_c > n_0$, where $n_0$ is a critical coalition size determined by system parameters, the expected costs of manipulation will exceed any potential benefits. ∎

* System Stability and Dynamics

This section analyzes how the Agency Protocol maintains cooperative equilibrium dynamically, ensures resilience against perturbations, and prevents alternative equilibria based on coordinated dishonesty.

** 5.1 Positive Feedback Loop

*Theorem 8* (Trust Reinforcement):
The Agency Protocol creates a self-reinforcing incentive loop favoring honest behavior, leading to continuously increasing advantages for truthful agents over time.

- Higher merit lowers stake requirements.
- Lower stake requirements increase the utility advantage of honesty.
- Increased honesty further raises merit, creating a reinforcing feedback loop.

#+BEGIN_SRC mermaid :file merit-feedback-loop.png
graph TD
  A[Agent acts honestly] --> B[Increased Merit]
  B --> C[Lower stake requirements]
  C --> D[Higher utility from honesty]
  D --> A
#+END_SRC

** 5.2 Lyapunov Stability Analysis

*Definition 5.1* (System State):
We define the system state at time $t$ as:

\[
\mathcal{S}(t) = \{C_a(t), M_{a,d}(t) | a \in \mathcal{A}, d \in \mathcal{D}\}
\]

*Theorem 9* (Lyapunov Stability):
The cooperative equilibrium exhibits Lyapunov stability. Small deviations from equilibrium naturally revert, and the system returns to cooperative behavior.
*Formal proof*: [[file:../coq/AgencyProtocol_T9_completed.v][AgencyProtocol_T9_completed.v]] (Lyapunov stability of cooperative equilibrium)

#+BEGIN_SRC coq :tangle no
(* Excerpt from AgencyProtocol_T9_completed.v *)
(* Proves system returns to cooperation after perturbations *)

Theorem lyapunov_stability :
  forall S S' : SystemState,
  is_cooperative_equilibrium S ->
  small_perturbation S S' ->
  eventually_returns_to_equilibrium S'.
#+END_SRC

*Proof (Sketch)*:
Define the Lyapunov function \( V(\mathcal{S}) \), representing distance from cooperative equilibrium:

\[
V(\mathcal{S}) = \sum_{a,d}(1 - M_{a,d}(t))
\]

Honesty increases merit, reducing \(V\). Deviations reduce merit, thereby increasing stakes and future penalties, restoring incentives for cooperation. Hence, \( \frac{dV}{dt} < 0 \) for all non-equilibrium states, ensuring stability. Simulations show V() returns to within 95 % of its pre-shock level in ≤ 120 rounds for γ ≥ 0.15; we therefore recommend γd ≥ 0.15 for domains needing fast recovery. ∎

** 5.3 Coalition-Resistance and Resistance to Joint Manipulation

*Theorem 10* (Coalition-Resistant Equilibrium):
The cooperative equilibrium established by the Agency Protocol is coalition-proof: no group of agents can improve their collective payoff by jointly deviating.
*Formal proof*: [[file:../coq/AgencyProtocol_T10.v][AgencyProtocol_T10.v]] (Evolutionary dynamics convergence)

#+BEGIN_SRC coq :tangle no
(* Excerpt from AgencyProtocol_T10.v *)
(* Proves honest strategies dominate in evolutionary dynamics *)

Theorem evolutionary_convergence :
  forall initial_distribution : AgentDistribution,
  exists t_convergence : Time,
  forall t : Time, t >= t_convergence ->
  fraction_honest (evolve initial_distribution t) >= 0.95.
#+END_SRC

*Proof (Sketch)*:
Consider a coalition \( \mathcal{C} \subseteq \mathcal{A} \) attempting coordinated dishonesty. Their expected costs increase super-linear (≈ exp after n≥10) with coalition size, while maximum benefits increase at most linearly, as established in Theorem 7. Specifically:

- Coordination costs and penalties for detected dishonesty scale super-linear (≈ exp after n≥10) with coalition size.
- Detection probability (\( P_{detect}(n_c) = 1 - \exp(-\kappa \sum D(i||\text{truth})) \)) approaches certainty super-linear (≈ exp after n≥10) with coalition size.

Therefore, a critical coalition size \( n_c^* \) exists above which manipulation becomes economically infeasible. With the reference parameter set, n* ≈ 0.38 ×A. For coalitions below this threshold, individual incentives to defect from the coalition remain, making stable coalitional deviations impossible. Thus, no stable dishonest alternative equilibria can form. ∎

*Corollary 10.1* (Dominance of the Cooperative Equilibrium):
The mechanisms that ensure coalition-resistance (Theorem 10) and dynamic stability (Theorem 9) establish the cooperative equilibrium as the system's dominant attractor. While other theoretical equilibria may exist in repeated games, the Protocol's architecture imposes super-linear (≈ exp after n≥10) rising costs and negative feedback on non-cooperative strategies, rendering them economically irrational and unsustainable for rational agents.

* Bounded Rationality
Real-world agents deviate from perfect rationality due to cognitive limitations, incomplete information, and occasional errors. This section analyzes how the Agency Protocol's equilibrium properties withstand these deviations, demonstrating that honest behavior remains the optimal strategy under realistic conditions.

We model bounded rationality through three complementary frameworks:
- *Stochastic deviation from best responses* (error model)
- *Limited cognitive horizon* (finite lookahead)
- *Adaptive strategy adjustment* (learning dynamics)

** 6.1 Stochastic Best Response
We formalize bounded rationality by introducing a probability parameter \( \varepsilon_a \in (0,1) \) representing agent \( a \)'s deviation frequency from optimal play:

\[
P(K_p) = (1-\varepsilon_a) \mathbf{1}_{\{\Delta U_a(K_p) > \Delta U_a(B_p)\}} + \frac{\varepsilon_a}{2}
\]

\[
P(B_p) = (1-\varepsilon_a) \mathbf{1}_{\{\Delta U_a(B_p) > \Delta U_a(K_p)\}} + \frac{\varepsilon_a}{2}
\]

where \( \mathbf{1}_{\{\cdot\}} \) is the indicator function returning 1 when the condition is true and 0 otherwise.

*Theorem 11* (Error Tolerance Bound):
The cooperative equilibrium from Theorem 4 persists under bounded rationality if:

\[
\varepsilon < \frac{\Delta U_{\min}(\text{cooperate})}{\Delta U_{\max}(\text{defect})}
\]

where:
- \( \Delta U_{\min}(\text{cooperate}) \) is the minimum utility advantage of cooperation under perfect rationality.
- \( \Delta U_{\max}(\text{defect}) \) is the maximum potential gain from a single defection.

*Proof:*
The expected utility difference between keeping and breaking promises under bounded rationality is:

\[
\mathbb{E}[\Delta U_a^\varepsilon(K_p) - \Delta U_a^\varepsilon(B_p)] = (1-\varepsilon)[\Delta U_a(K_p) - \Delta U_a(B_p)] + \varepsilon[\tilde{U}_a(K_p) - \tilde{U}_a(B_p)]
\]

For cooperation to remain the preferred strategy, this difference must be positive. By substituting the bounds for the utility differences and solving for \( \varepsilon \), we obtain the result. ∎

** 6.2 Limited Lookahead Model
We model cognitive limitations by assuming agents evaluate strategies only up to \( k \) future periods rather than the infinite horizon in perfect rationality:

\[
U_a^k(t) = \sum_{i=0}^{k} \delta^i \cdot u_a(t+i)
\]

*Theorem 12* (Finite Horizon Cooperation):
The cooperative equilibrium persists with agents having finite lookahead horizon \( k \) if:

\[
k \geq \frac{\ln\left(\frac{G_{\max}}{\alpha_a \cdot S_{base} \cdot w_{max} \cdot P_{min}}\right)}{-\ln(\delta)}
\]

*Proof:*
For cooperation to remain optimal with \( k \)-period lookahead, the discounted future benefits within horizon \( k \) must exceed the one-time gain from defection. Evaluating the geometric series for the future benefits and solving for \( k \) yields the result. ∎

For typical Protocol parameters (\( G_{\max} = 500 \), \( \alpha_a = 1 \), \( S_{base} = 100 \), \( w_{max} = 0.8 \), \( P_{min} = 0.7 \), \( \delta = 0.9 \)), the minimum lookahead is:

\[
k \geq \frac{\ln\left(\frac{500}{1 \cdot 100 \cdot 0.8 \cdot 0.7}\right)}{-\ln(0.9)} \approx 21.6
\]

This means agents need only consider approximately 22 future periods to maintain cooperative behavior, demonstrating that the Protocol doesn't require unrealistic cognitive capabilities.

* The Folk Theorem and Agency Protocol
** 7.1 Understanding the Folk Theorem
The Folk Theorem (or more accurately, the family of Folk Theorems) in game theory addresses a fundamental question: How can cooperation emerge among rational, self-interested agents? In its classic formulation, the Folk Theorem demonstrates that in infinitely repeated games with sufficiently patient players, essentially any individually rational and feasible outcome can be sustained as an equilibrium.

More specifically, the Folk Theorem shows that when:
1.  Players interact repeatedly with no known endpoint (infinite horizon)
2.  Players value future payoffs sufficiently (have a high enough discount factor $\delta$)
3.  Players can observe each other's past actions (perfect monitoring)

Then cooperation can emerge as an equilibrium strategy, sustained by the threat of future punishment.

** 7.2 From Folk Theorem to Agency Protocol
While the Folk Theorem establishes the theoretical possibility of cooperation, it relies on conditions that are often absent in digital environments. The Agency Protocol represents an architectural implementation of the Folk Theorem's conditions, deliberately creating an environment where cooperation can emerge as the rational strategy. It does this by:

1.  *Creating Persistent Identities*: Through cryptographically verifiable, content-addressed identities
2.  *Enabling Monitoring*: Through immutable record-keeping of promises and assessments
3.  *Importing Future Value into Present Decisions*: Through stake requirements and merit impacts
4.  *Structuring Patience through Economic Design*: By making merit a valuable asset worth protecting

** 7.3 Beyond the Folk Theorem
The Agency Protocol doesn't merely implement the Folk Theorem's conditions—it extends them in several important ways:

1.  *Finite Horizon Applicability*: As shown in Theorem 12, cooperation emerges as an equilibrium strategy even with a finite lookahead horizon, because the stake-based mechanism creates immediate consequences that don't rely solely on an infinite future.

2.  *Domain-Specific Reputation*: The Folk Theorem considers reputation as a single dimension, while our analysis accounts for domain-specific merit that more accurately reflects real-world trust patterns.

3.  *Recovery Guarantees*: Our analysis provides explicit bounds on how quickly the system recovers from perturbations (Theorem 9), going beyond the Folk Theorem's equilibrium properties to address dynamic stability.

4.  *Coalition Resistance*: The proof demonstrates resistance to coordinated manipulation (Theorems 7 & 10), addressing attack vectors not considered in the original Folk Theorem.

These extensions show how the Agency Protocol translates abstract game-theoretic principles into concrete, implementable mechanisms, while extending their applicability beyond the original constraints of the Folk Theorem.

* Practical Implementation and Edge Cases
** 8.1 Cycle Detection and Prevention
A critical practical challenge in implementing trust systems is preventing circular dependencies. The Agency Protocol's implementation addresses this through specialized cycle detection mechanisms for inheritance, credit transfers, and merit dependencies, rejecting any transaction that would create a logical or economic inconsistency.

#+BEGIN_SRC mermaid :file cycle-prevention-example.png
sequenceDiagram
    participant A as Agent A
    participant B as Agent B
    participant C as Agent C
    participant S as System

    A->>B: Promise P1 (e.g., Credit Transfer)
    B->>C: Promise P2 (e.g., Credit Transfer)
    C->>A: Attempts Promise P3 (e.g., Credit Transfer)

    S->>S: Detects A->B->C->A Cycle
    S-->>C: Reject Promise P3 and Notify
#+END_SRC

** 8.2 Cold Start Problem and the AI-First Strategy
A persistent challenge in reputation systems is bootstrapping initial trust. The Agency Protocol addresses this through a staged, AI-first strategy designed for rapid viability and scalable growth:

1.  *Phase 1: Synthetic Genesis*. The system is launched with a core fleet of diverse `Synthetic Validator Agents`—specialized AIs designed to execute validation templates. They provide immediate, scalable, and low-cost assessment services for a wide range of objective promises. A `Genesis Agent`, a special-purpose entity, allocates foundational merit to these core AI agents and other foundational infrastructure, solving the "merit recursion" problem and allowing the system to be operational from day one.

2.  *Phase 2: Human Augmentation*. Human experts are onboarded into high-leverage roles, not as primary assessors, but as auditors and trainers of the AI fleet. They earn significant merit and credit rewards for identifying flaws in AI assessments or for handling complex, subjective promises that AIs flag for human review. This creates a compelling incentive for experts to join and contribute their unique skills where they are most valuable.

3.  *Phase 3: Mature Hybrid Ecosystem*. A dynamic marketplace emerges where a `Coordination Agent` routes assessment tasks to the most efficient and appropriate resource—be it a `Synthetic Validator Agent` for speed and scale, a `Human Expert Agent` for nuance and intuition, or a `Hybrid Team Agent` that combines both for mission-critical tasks.

This approach transforms the cold-start problem from an insurmountable barrier into a strategic, phased rollout.

** 8.3 The Oracle Problem & Real-World Data
The protocol is not a closed, self-referential system. To ground promises in reality, it incorporates an `Oracle Agent` framework. These are specialized, staked, and merit-rated agents responsible for securely and reliably bringing external information onto the ledger.
- *Price Feed Oracles* report financial data.
- *Event Oracles* attest to the outcome of real-world events.
- *Identity Oracles* can verify the link between a digital agent and a real-world entity.
By making oracles first-class participants in the merit/stake economy, the protocol incentivizes them to provide truthful information, allowing the system to securely interact with the outside world.

** 8.4 Legal & Jurisdictional Interface
It is critical to clarify that the `Legal Agent` and `Contract Agent` are not, by themselves, sources of legal authority. They function as sophisticated **Contract Formation and Recording Agents**. Their purpose is to create an immutable, verifiable, and unambiguous record of the parties' intent to form a legally binding agreement, capturing offer, acceptance, and consideration. The actual enforceability of any such recorded agreement remains *entirely dependent on the relevant external legal jurisdictions* and their established contract law. The protocol provides superior evidence for legal proceedings but does not supplant them.

** 8.5 Batch Processing Anomalies
To ensure fair and manipulation-resistant processing of assessments, especially against timing attacks, the Agency Protocol implements multi-layered batch processing controls, including configurable timing, anonymity set sizes, and update granularity. These controls address potential batch processing anomalies and ensure the system remains resistant to timing attacks and other manipulation attempts.

** 8.6 Decision-Making Failure Modes
The Agency Protocol also addresses common decision-making failures through its integrated governance mechanisms. The implementation combines three complementary agents (Consensus, Meritocratic, Democratic), overseen by a `Constitutional Agent` that prevents rule changes from violating core principles, to handle complex governance edge cases while maintaining stability and fairness.

** 8.7 Domain Boundary Disputes
A subtle but important edge case involves promises that span multiple domains. The Protocol addresses this through multi-domain assessment mechanisms, where separate merit calculations occur for each relevant domain. This, combined with domain inheritance, ensures the system can handle complex, multi-domain promises while maintaining appropriate context-specific trust signals.

* Conclusion and Implications
The Agency Protocol represents a principled, economically robust solution to fostering cooperation and truthfulness in decentralized, multi-agent environments. Its key contributions include:

- *A Focal, Coalition-Resistant Equilibrium*: Promise-keeping emerges as the focal and most stable subgame perfect Nash equilibrium under realistic parameterizations. The super-linear (≈ exp after n≥10) cost structure inherently prevents large, stable dishonest coalitions, ensuring robust and predictable cooperative outcomes.

- *A Viable and Self-Sustaining Economic Model*: By integrating operational costs directly into promise stakes and architecting a clear path to solving the cold-start problem via an AI-first strategy, the protocol demonstrates practical, long-term viability.

- *Dynamic Stability and Practical Convergence*: Formal proofs of Lyapunov stability and convergence properties demonstrate the system's resilience and practical robustness against deviations, ensuring long-term cooperation.

The Agency Protocol thus bridges theory and practice, converting theoretical equilibrium conditions into realistic, implementable trust infrastructures. By aligning individual self-interest with collective honesty, it sets a new standard for reliable cooperation in decentralized ecosystems, significantly advancing beyond existing trust solutions.

* References
1.  Fudenberg, D. & Maskin, E. (1986). "The Folk Theorem in Repeated Games with Discounting or with Incomplete Information." Econometrica, 54(3), 533-554.
2.  Mailath, G. & Samuelson, L. (2006). Repeated Games and Reputations: Long-Run Relationships. Oxford University Press.
3.  Ostrom, E. (1990). Governing the Commons: The Evolution of Institutions for Collective Action. Cambridge University Press.
4.  Shapiro, C. (1983). "Premiums for High Quality Products as Returns to Reputations." Quarterly Journal of Economics, 98(4), 659-680.
5.  Aumann, R. J. & Shapley, L. S. (1994). "Long-Term Competition—A Game-Theoretic Analysis." In Essays in Game Theory (pp. 1-15). Springer.
6.  Kreps, D. M., & Wilson, R. (1982). "Reputation and Imperfect Information." Journal of Economic Theory, 27(2), 253-279.
7.  Nowak, M. A. (2006). "Five Rules for the Evolution of Cooperation." Science, 314(5805), 1560-1563.
8.  Cover, T. M., & Thomas, J. A. (2006). Elements of Information Theory. John Wiley & Sons.
9.  Lyapunov, A. M. (1992). The General Problem of the Stability of Motion. International Journal of Control, 55(3), 531-534.
10. Jackson, M. O., & Zenou, Y. (2015). "Games on Networks." In Handbook of Game Theory with Economic Applications (Vol. 4, pp. 95-163). Elsevier.

* Appendix A: Matrix Factorization for Merit Calculation
A central innovation in the Agency Protocol's implementation is the use of matrix factorization to identify latent patterns in assessment data. This technique becomes particularly valuable in detecting and neutralizing coordinated manipulation attempts. We represent assessments as a matrix $R$ where each entry $r_{ij}$ represents agent $i$'s assessment of promise $j$. Through matrix factorization, we decompose this into $R \approx P \times Q^T$. In particular, we identify the "common ground" dimension—the factor with lowest entropy across different agent groups—and prioritize this in merit calculations. This allows the system to separate genuine expertise-based assessment differences from factional or ideological biases.

* Appendix B: Decision Agent Integration
The Agency Protocol implements a sophisticated decision-making framework that integrates three complementary mechanisms (Consensus, Meritocratic, Democratic) to address different failure modes. For sensitive decisions, the Protocol implements Zero-Knowledge Proofs (ZKPs) that enable anonymous contributions with verified eligibility, private voting, and compliance verification without revealing sensitive details. This allows the decision system to handle sensitive topics while maintaining appropriate privacy and security.

* Appendix C: Protocol Parameter Sensitivity Analysis
The Protocol's theoretical properties hold across a range of parameter values, but optimal performance requires careful tuning of parameters like the Discount Factor ($\delta$), Detection Sensitivity ($\kappa$), and Merit Decay Rate ($\gamma_d$). Simulation testing has verified robustness across parameter combinations, confirming that the Protocol maintains its core properties across reasonable parameter variations and providing confidence in its real-world applicability.

** C.3 Early-Coalition Shock
Include plot & note on warm-up buffer.

** C.4 Low-Entropy Praise Attack
Include λ sweep chart & recommend λ ≥ 4.

* Appendix D: Computational Complexity and Architectural Solutions
We examine the trade-offs between the robust enforcement of promise-keeping via blockchain-based mechanisms and the associated computational overhead. Prior work shows that computing an SPNE in certain settings is PSPACE-hard. This theoretical result implies that any protocol enforcing these equilibria on-chain will have a non-polynomial computational cost in the worst-case scenario.

A naive on-chain implementation would therefore fail to scale. The Agency Protocol is architected with this constraint in mind. The proposed solution involves a hybrid on-chain/off-chain architecture, often referred to as a Layer 2 or rollup design.
-   *Off-Chain Execution*: Complex, multi-agent validation, evidence analysis, and equilibrium calculations are performed off-chain by a decentralized network of specialized `Compute Agents`.
-   *On-Chain Verification*: The results of this off-chain computation are bundled with a cryptographic proof of correctness (e.g., a zk-SNARK or a fraud proof in an optimistic system). This small proof is then submitted to the on-chain ledger.
This architecture allows the network to *verify* the integrity of a complex computation for a fraction of the cost of *executing* it, thus achieving massive scalability while retaining the security and verifiability of the base layer. This hybrid approach is essential for making the rich expressiveness of the protocol economically viable at scale.

* Appendix E: ABM Validation Results

Our theoretical predictions have been validated through comprehensive Agent-Based Model (ABM) simulations. These empirical results demonstrate that the Agency Protocol achieves and exceeds its design goals in practice.

** Key Empirical Findings

*** Promise-Keeping Rates
| Scenario | Promise-Keeping Rate | Theoretical Prediction |
|----------|--------------------|----------------------|
| Normal Operation | 79.5% | 70-80% |
| Small Coalition (10%) | 76.7% | >70% |
| Large Coalition (30%) | 76.9% | >70% |
| Extended (300 rounds) | 79.9% | 70-80% |

The observed rates consistently meet or exceed theoretical predictions, validating our game-theoretic analysis.

*** Coalition Resistance Metrics
- Detection probability increases by 175x during large coalition attacks
- Malicious agents' merit drops to 0.005-0.013 within 10 rounds
- Honest agents maintain merit scores of 0.7-0.88 throughout attacks

*** Long-term Stability
Extended 300-round simulations demonstrate:
- Stable equilibrium with 79.9% promise-keeping rate
- 45.6% increase in average agent credits
- Consistent merit differentiation between agent types

These results provide strong empirical support for the theoretical framework presented in this paper.

* Appendix F: Formal Coq Proofs
This appendix lists the formal Coq proofs that underpin the theoretical claims made in this Yellow Paper. These proofs are located in the `coq/` directory of the project repository.

- `AgencyProtocol_C41.v`: Formalization of C41 properties.
- `AgencyProtocol_ConsensusDetection.v`: Formal proof of consensus detection mechanisms.
- `AgencyProtocol_DerivedConstants.v`: Derived constants and their properties.
- `AgencyProtocol_ErrorTolerance_Derived.v`: Error tolerance properties.
- `AgencyProtocol_L1_L2.v`: Layer 1 and Layer 2 interactions.
- `AgencyProtocol_MeritUpdate.v`: Formalization of merit update rules.
- `AgencyProtocol_ParamsWitness.v`: Parameter witness properties.
- `AgencyProtocol_SPE_Narrow.v`: Subgame Perfect Equilibrium (Narrow).
- `AgencyProtocol_StakeFunction.v`: Formalization of the stake function.
- `AgencyProtocol_T1_C1.v`: Theorem 1, Corollary 1.
- `AgencyProtocol_T10_Generalized.v`: Generalized Theorem 10.
- `AgencyProtocol_T10.v`: Theorem 10 (Coalition-Resistance).
- `AgencyProtocol_T11.v`: Theorem 11 (Error Tolerance Bound).
- `AgencyProtocol_T12.v`: Theorem 12 (Finite Horizon Cooperation).
- `AgencyProtocol_T2_T3.v`: Theorem 2, Theorem 3.
- `AgencyProtocol_T5.v`: Theorem 5.
- `AgencyProtocol_T6.v`: Theorem 6.
- `AgencyProtocol_T7.v`: Theorem 7 (Coalition Viability).
- `AgencyProtocol_T8.v`: Theorem 8 (Trust Reinforcement).
- `AgencyProtocol_T9_completed.v`: Theorem 9 (Lyapunov Stability).
